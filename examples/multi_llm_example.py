#!/usr/bin/env python3
"""
å¤šLLMä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•åœ¨DeepDive Analysté¡¹ç›®ä¸­ä½¿ç”¨ä¸åŒçš„LLMæä¾›å•†
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.llm.llm_factory import LLMFactory
from src.configs.config import Config
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.prompt import Prompt, Confirm

console = Console()


def show_llm_providers():
    """æ˜¾ç¤ºæ”¯æŒçš„LLMæä¾›å•†"""
    console.print("[bold blue]ğŸ¤– æ”¯æŒçš„LLMæä¾›å•†[/bold blue]")
    
    providers = LLMFactory.get_supported_providers()
    
    table = Table(title="LLMæä¾›å•†ä¿¡æ¯")
    table.add_column("æä¾›å•†", style="cyan")
    table.add_column("æè¿°", style="green")
    table.add_column("å¯ç”¨æ¨¡å‹", style="yellow")
    
    for provider in providers:
        try:
            info = LLMFactory.get_provider_info(provider)
            models = info['available_models'][:3]  # åªæ˜¾ç¤ºå‰3ä¸ªæ¨¡å‹
            models_str = ', '.join(models) + ('...' if len(info['available_models']) > 3 else '')
            
            table.add_row(
                provider.upper(),
                info['description'],
                models_str
            )
        except Exception as e:
            table.add_row(provider.upper(), "è·å–ä¿¡æ¯å¤±è´¥", str(e))
    
    console.print(table)


def test_llm_provider(provider: str, model: str, api_key: str):
    """æµ‹è¯•LLMæä¾›å•†"""
    console.print(f"\n[bold blue]ğŸ§ª æµ‹è¯• {provider.upper()} æä¾›å•†[/bold blue]")
    
    try:
        # åˆ›å»ºLLMå®ä¾‹
        llm = LLMFactory.create_llm(
            provider=provider,
            model=model,
            api_key=api_key,
            temperature=0.1,
            max_tokens=100
        )
        
        console.print(f"[green]âœ…[/green] LLMå®ä¾‹åˆ›å»ºæˆåŠŸ: {llm}")
        
        # æµ‹è¯•ç”Ÿæˆ
        test_prompt = "è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ã€‚"
        console.print(f"[yellow]ğŸ“[/yellow] æµ‹è¯•æç¤º: {test_prompt}")
        
        response = llm.generate(test_prompt)
        
        if response.success:
            console.print(f"[green]âœ…[/green] ç”ŸæˆæˆåŠŸ!")
            console.print(f"[blue]ğŸ“„[/blue] å“åº”å†…å®¹: {response.content}")
            
            if response.usage:
                console.print(f"[blue]ğŸ“Š[/blue] Tokenä½¿ç”¨: {response.usage}")
        else:
            console.print(f"[red]âŒ[/red] ç”Ÿæˆå¤±è´¥: {response.error_message}")
            
        return response.success
        
    except Exception as e:
        console.print(f"[red]âŒ[/red] æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


def interactive_llm_test():
    """äº¤äº’å¼LLMæµ‹è¯•"""
    console.print("\n[bold blue]ğŸ¯ äº¤äº’å¼LLMæµ‹è¯•[/bold blue]")
    
    # æ˜¾ç¤ºæä¾›å•†
    show_llm_providers()
    
    # è·å–ç”¨æˆ·é€‰æ‹©
    providers = LLMFactory.get_supported_providers()
    console.print("\nè¯·é€‰æ‹©è¦æµ‹è¯•çš„LLMæä¾›å•†:")
    for i, provider in enumerate(providers, 1):
        console.print(f"{i}. {provider.upper()}")
    
    while True:
        try:
            choice = int(Prompt.ask("è¯·è¾“å…¥é€‰æ‹©", default="1"))
            if 1 <= choice <= len(providers):
                selected_provider = providers[choice - 1]
                break
            else:
                console.print(f"[red]âŒ[/red] è¯·è¾“å…¥1-{len(providers)}ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            console.print("[red]âŒ[/red] è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    
    # è·å–æ¨¡å‹é€‰æ‹©
    try:
        models = LLMFactory.get_available_models(selected_provider)
        console.print(f"\n{selected_provider.upper()} å¯ç”¨æ¨¡å‹:")
        for i, model in enumerate(models, 1):
            console.print(f"{i}. {model}")
        
        while True:
            try:
                model_choice = int(Prompt.ask("è¯·é€‰æ‹©æ¨¡å‹", default="1"))
                if 1 <= model_choice <= len(models):
                    selected_model = models[model_choice - 1]
                    break
                else:
                    console.print(f"[red]âŒ[/red] è¯·è¾“å…¥1-{len(models)}ä¹‹é—´çš„æ•°å­—")
            except ValueError:
                console.print("[red]âŒ[/red] è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    except Exception as e:
        console.print(f"[yellow]âš ï¸[/yellow] æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨: {str(e)}")
        selected_model = Prompt.ask("è¯·è¾“å…¥æ¨¡å‹åç§°")
    
    # è·å–APIå¯†é’¥
    api_key = Prompt.ask(f"è¯·è¾“å…¥ {selected_provider.upper()} APIå¯†é’¥", password=True)
    
    if not api_key:
        console.print("[red]âŒ[/red] æœªæä¾›APIå¯†é’¥")
        return
    
    # æ‰§è¡Œæµ‹è¯•
    success = test_llm_provider(selected_provider, selected_model, api_key)
    
    if success:
        console.print(f"\n[green]ğŸ‰[/green] {selected_provider.upper()} æµ‹è¯•æˆåŠŸ!")
    else:
        console.print(f"\n[red]ğŸ’¥[/red] {selected_provider.upper()} æµ‹è¯•å¤±è´¥!")


def test_current_config():
    """æµ‹è¯•å½“å‰é…ç½®çš„LLM"""
    console.print("\n[bold blue]âš™ï¸ æµ‹è¯•å½“å‰é…ç½®çš„LLM[/bold blue]")
    
    try:
        # è·å–å½“å‰é…ç½®
        llm_config = Config.get_llm_config()
        
        console.print(f"[cyan]å½“å‰é…ç½®:[/cyan]")
        console.print(f"  æä¾›å•†: {llm_config['provider']}")
        console.print(f"  æ¨¡å‹: {llm_config['model']}")
        console.print(f"  APIå¯†é’¥: {'å·²é…ç½®' if llm_config['api_key'] else 'æœªé…ç½®'}")
        
        if not llm_config['api_key']:
            console.print("[red]âŒ[/red] å½“å‰é…ç½®ç¼ºå°‘APIå¯†é’¥")
            return
        
        # æµ‹è¯•å½“å‰é…ç½®
        success = test_llm_provider(
            llm_config['provider'],
            llm_config['model'],
            llm_config['api_key']
        )
        
        if success:
            console.print(f"\n[green]ğŸ‰[/green] å½“å‰LLMé…ç½®æµ‹è¯•æˆåŠŸ!")
        else:
            console.print(f"\n[red]ğŸ’¥[/red] å½“å‰LLMé…ç½®æµ‹è¯•å¤±è´¥!")
            
    except Exception as e:
        console.print(f"[red]âŒ[/red] æµ‹è¯•å½“å‰é…ç½®å¤±è´¥: {str(e)}")


def compare_providers():
    """æ¯”è¾ƒä¸åŒæä¾›å•†"""
    console.print("\n[bold blue]ğŸ“Š æ¯”è¾ƒä¸åŒLLMæä¾›å•†[/bold blue]")
    
    # è¿™é‡Œå¯ä»¥å®ç°ä¸€ä¸ªç®€å•çš„æ¯”è¾ƒæµ‹è¯•
    # ä½¿ç”¨ç›¸åŒçš„æç¤ºè¯æµ‹è¯•ä¸åŒçš„æä¾›å•†
    
    test_prompt = "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªç®€å•çš„ä¾‹å­ã€‚"
    console.print(f"[yellow]ğŸ“[/yellow] æµ‹è¯•æç¤º: {test_prompt}")
    
    # è¿™é‡Œéœ€è¦ç”¨æˆ·æä¾›å¤šä¸ªAPIå¯†é’¥
    console.print("\n[yellow]ğŸ’¡[/yellow] è¦æ¯”è¾ƒä¸åŒæä¾›å•†ï¼Œè¯·ç¡®ä¿åœ¨.envæ–‡ä»¶ä¸­é…ç½®äº†å¤šä¸ªAPIå¯†é’¥")
    console.print("ç„¶åå¯ä»¥è¿è¡Œ: python main.py research --query 'ä½ çš„æŸ¥è¯¢' --verbose")


def show_usage_examples():
    """æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹"""
    console.print("\n[bold blue]ğŸ“š ä½¿ç”¨ç¤ºä¾‹[/bold blue]")
    
    examples = [
        {
            "åœºæ™¯": "ä½¿ç”¨OpenAI GPT-4",
            "é…ç½®": "LLM_PROVIDER=openai\nLLM_MODEL=gpt-4o\nOPENAI_API_KEY=your_key",
            "å‘½ä»¤": "python main.py research --query 'å¯¹æ¯”Reactå’ŒVue'"
        },
        {
            "åœºæ™¯": "ä½¿ç”¨Google Gemini",
            "é…ç½®": "LLM_PROVIDER=gemini\nLLM_MODEL=gemini-1.5-pro\nGEMINI_API_KEY=your_key",
            "å‘½ä»¤": "python main.py research --query 'æ·±å…¥è§£é‡ŠDocker'"
        },
        {
            "åœºæ™¯": "ä½¿ç”¨é˜¿é‡Œé€šä¹‰åƒé—®",
            "é…ç½®": "LLM_PROVIDER=qwen\nLLM_MODEL=qwen-max\nQWEN_API_KEY=your_key",
            "å‘½ä»¤": "python main.py research --query 'ç›˜ç‚¹æœºå™¨å­¦ä¹ æ¡†æ¶'"
        },
        {
            "åœºæ™¯": "ä½¿ç”¨Anthropic Claude",
            "é…ç½®": "LLM_PROVIDER=anthropic\nLLM_MODEL=claude-3-5-sonnet-20241022\nANTHROPIC_API_KEY=your_key",
            "å‘½ä»¤": "python main.py research --query 'å¦‚ä½•ä½¿ç”¨Kubernetes'"
        }
    ]
    
    table = Table(title="LLMä½¿ç”¨ç¤ºä¾‹")
    table.add_column("åœºæ™¯", style="cyan")
    table.add_column("é…ç½®", style="green")
    table.add_column("å‘½ä»¤", style="yellow")
    
    for example in examples:
        table.add_row(
            example["åœºæ™¯"],
            example["é…ç½®"],
            example["å‘½ä»¤"]
        )
    
    console.print(table)


def main():
    """ä¸»å‡½æ•°"""
    console.print(Panel.fit(
        "[bold blue]å¤šLLMä½¿ç”¨ç¤ºä¾‹[/bold blue]\n"
        "æ¼”ç¤ºDeepDive Analysté¡¹ç›®çš„å¤šLLMæä¾›å•†æ”¯æŒ",
        title="æ¬¢è¿",
        border_style="blue"
    ))
    
    while True:
        console.print("\n[bold blue]è¯·é€‰æ‹©æ“ä½œ:[/bold blue]")
        console.print("1. æ˜¾ç¤ºLLMæä¾›å•†ä¿¡æ¯")
        console.print("2. æµ‹è¯•å½“å‰é…ç½®çš„LLM")
        console.print("3. äº¤äº’å¼LLMæµ‹è¯•")
        console.print("4. æ¯”è¾ƒä¸åŒæä¾›å•†")
        console.print("5. æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹")
        console.print("6. é€€å‡º")
        
        choice = Prompt.ask("è¯·è¾“å…¥é€‰æ‹© (1-6)", default="1")
        
        if choice == "1":
            show_llm_providers()
        elif choice == "2":
            test_current_config()
        elif choice == "3":
            interactive_llm_test()
        elif choice == "4":
            compare_providers()
        elif choice == "5":
            show_usage_examples()
        elif choice == "6":
            console.print("[green]ğŸ‘‹[/green] å†è§!")
            break
        else:
            console.print("[red]âŒ[/red] æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
        
        if choice != "6":
            if not Confirm.ask("æ˜¯å¦ç»§ç»­ï¼Ÿ", default=True):
                break
    
    console.print("\n[green]âœ…[/green] ç¤ºä¾‹æ¼”ç¤ºå®Œæˆ!")
    console.print("[blue]ğŸ’¡[/blue] æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹:")
    console.print("  - é…ç½®ç®¡ç†: python scripts/setup_llm.py")
    console.print("  - LLMä¿¡æ¯: python main.py llm")
    console.print("  - é…ç½®æ£€æŸ¥: python main.py config")


if __name__ == "__main__":
    main()
